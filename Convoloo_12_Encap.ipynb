{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kegong20242024/ML-Therapy-Prediction/blob/main/Convoloo_12_Encap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGfflo2fVvZ7"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()from google.colab import drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "df = pd.read_excel('/content/drive/MyDrive/studiendaten_f__r_similarity_analyse_neu.xlsx', engine='openpyxl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aQew5S0V4hO"
      },
      "outputs": [],
      "source": [
        "!pip install lazypredict\n",
        "!pip install scikit-multilearn\n",
        "!pip install xgboost\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.multioutput import MultiOutputClassifier, ClassifierChain\n",
        "from skmultilearn.problem_transform import LabelPowerset\n",
        "\n",
        "from sklearn.metrics import accuracy_score, hamming_loss, make_scorer\n",
        "\n",
        "from itertools import combinations\n",
        "from collections import Counter\n",
        "import io\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_UfpgvSV5Ml"
      },
      "outputs": [],
      "source": [
        "# df = pd.read_excel('studiendaten_f__r_similarity_analyse_neu.xlsx', engine='openpyxl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98f0CHlYV_R4"
      },
      "source": [
        "Data Preprocessing:\n",
        "- Invalid Char to NaN\n",
        "- Values & Column formalization\n",
        "- Remove non-related columns\n",
        "  - Eingangsbuvhnumer: serial number\n",
        "  - Progress: info duplicate\n",
        "  - Anzahl Metastasen extrahepatisch: all NaN values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4NjjbDaV63F"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRj4J-Z9eGNP"
      },
      "outputs": [],
      "source": [
        "#\n",
        "\n",
        "\n",
        "col1 = 'Wirkstoffschema neoadjuvante Therapie'\n",
        "col2 = 'Wirkstoffschema adjuvante Therapie'\n",
        "lower = [s.strip().lower() for s in [\n",
        "    'entfällt', 'fehlt', 'entfällt/fehlt', 'n.r.', 'enfällt', 'k.a.', 'k.a', 'keine angabe'\n",
        "]]\n",
        "unwanted = ['fehlt']\n",
        "\n",
        "\n",
        "def clean_except(df, lower, unwanted, col1, col2):\n",
        "    def clean_cell_general(x):\n",
        "\n",
        "        if isinstance(x, str):\n",
        "            return x.strip().lower()\n",
        "        return x\n",
        "\n",
        "    def clean_cell_with_nulling(x):\n",
        "\n",
        "        if isinstance(x, str):\n",
        "            x_cleaned = x.strip().lower()\n",
        "            return None if x_cleaned in lower or x_cleaned in unwanted else x_cleaned\n",
        "        return x\n",
        "\n",
        "    for col in df.columns:\n",
        "        if col in [col1, col2]:\n",
        "            df[col] = df[col].apply(clean_cell_general)\n",
        "        else:\n",
        "            df[col] = df[col].apply(clean_cell_with_nulling)\n",
        "    return df\n",
        "\n",
        "\n",
        "df = clean_except(df, lower, unwanted, col1, col2)\n",
        "\n",
        "\n",
        "columns_to_drop = ['Eingangsbuchnummer', 'Progress', 'Anzahl Metastasen extrahepatisch']\n",
        "df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
        "\n",
        "\n",
        "df.columns = df.columns.str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8zspfdo82Lc"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pmN1CPuXBkm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72S-vbWgXyYC"
      },
      "source": [
        "NaN value visualization Bar Chart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8N4oasUlXYTf"
      },
      "outputs": [],
      "source": [
        "# missing val visualization\n",
        "nan_counts = df.isna().sum()\n",
        "nan_counts = nan_counts[nan_counts > 0]\n",
        "\n",
        "plt.figure(figsize=(14, 16))\n",
        "sns.barplot(x=nan_counts.index, y=nan_counts.values, color='steelblue')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title('Missing Value Distribution per Column')\n",
        "plt.xlabel('Column')\n",
        "plt.ylabel('Number of Missing Values')\n",
        "plt.ylim(0, nan_counts.max() * 1.3)\n",
        "for i, val in enumerate(nan_counts.values):\n",
        "    plt.text(i, val + 10, str(val), ha='center', va='bottom', fontsize=8, rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3JJSltEX9QG"
      },
      "source": [
        "Dealing NaN value: cleaning Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEUO-fYgX1bv"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_filled = df.copy()\n",
        "\n",
        "# Delete the rows where both are empty (including 'nein', 'keine')\n",
        "col1 = 'Wirkstoffschema neoadjuvante Therapie'\n",
        "col2 = 'Wirkstoffschema adjuvante Therapie'\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsTtiDSLjpq3"
      },
      "source": [
        "Dealing NAN values: Median & Mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3HwR_80j0RU"
      },
      "outputs": [],
      "source": [
        "# Median\n",
        "numerical_median_fill = [\n",
        "    'Alter des Patienten bei Diagnose', 'BMI', 'PFS', 'OS',\n",
        "    'Tumor/Metastasen Durchmesser', 'Anzahl Metastasen intrahepatisch',\n",
        "    'Anzahl Metastasen extrahepatisch'\n",
        "]\n",
        "\n",
        "for col in numerical_median_fill:\n",
        "    if col in df_filled.columns:\n",
        "        df_filled[col] = pd.to_numeric(df_filled[col], errors='coerce')\n",
        "        median_val = df_filled[col].median()\n",
        "        df_filled[col] = df_filled[col].fillna(median_val)\n",
        "\n",
        "# Mode\n",
        "categorical_mode_fill = [\n",
        "    'T-Status', 'N-Status', 'M-Status',\n",
        "    'V-Status', 'L-Status', 'Pn-Status','Grading', 'R-Status',\n",
        "    'Tumorseite', 'Histologie'\n",
        "    , 'Vitalstatus','Tumorbedingt verstorben', 'synchrone oder metachrone Metastasierung',\n",
        "    'Tumormarker 1', 'Tumormarker 2','Therapieerfolg neoadjuvante Therapie', 'adjuvante Chemotherapie',\n",
        "    'Vorerkrankungen','early Progress 1=nein; 2= ja', 'neoadjuvante Chemotherapie'\n",
        "]\n",
        "\n",
        "for col in categorical_mode_fill:\n",
        "    if col in df_filled.columns:\n",
        "        mode_val = df_filled[col].mode()[0]\n",
        "        df_filled[col] = df_filled[col].fillna(mode_val)\n",
        "\n",
        "#Drop Colume(since cannot estimate date & missing > 250)\n",
        "drop_col = ['Geburtsdatum', 'Progress Datum','OP Datum', 'Nachsorge Datum', 'Todesdatum', 'Ort metastasen extrahepatisch',\n",
        "            'Adjuvante Therapie des Primärtumors oder vorhergeganener Metastase','Wirkstoffschema der Adjuvanten Therapie des Primärtumors oder vorhergegangener Metastase', 'Pn-Status']\n",
        "df_filled = df_filled.drop(columns=[col for col in drop_col if col in df.columns])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKbwhNGZj-on"
      },
      "source": [
        "Check Any NAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6N534soSkAyo"
      },
      "outputs": [],
      "source": [
        "total_missing = df_filled.isna().sum().sum()\n",
        "if total_missing == 0:\n",
        "    print(\"no NAN\")\n",
        "else:\n",
        "    print(f\"{total_missing} NAN\")\n",
        "# missing val visualization\n",
        "# total_missing = df_filled.isna().sum().sum()\n",
        "# if total_missing == 0:\n",
        "#     print(\"no NAN\")\n",
        "# else:\n",
        "#     print(f\"{total_missing} NAN\")\n",
        "\n",
        "nan_counts = df_filled.isna().sum()\n",
        "nan_counts = nan_counts[nan_counts > 0]\n",
        "\n",
        "# plt.figure(figsize=(14, 16))\n",
        "# sns.barplot(x=nan_counts.index, y=nan_counts.values, color='salmon')\n",
        "# plt.xticks(rotation=45, ha='right')\n",
        "# plt.title('Remaining Missing Value Distribution per Column')\n",
        "# plt.xlabel('Column')\n",
        "# plt.ylabel('Number of Missing Values')\n",
        "# plt.ylim(0, nan_counts.max() * 1.3)\n",
        "# for i, val in enumerate(nan_counts.values):\n",
        "#     plt.text(i, val + 5, str(val), ha='center', va='bottom', fontsize=8, rotation=90)\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIBRmVtJkJFV"
      },
      "source": [
        "Save to filled_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ko-MrZGykRsa"
      },
      "outputs": [],
      "source": [
        "df_filled.to_excel(\"filled_dataset.xlsx\", index=False)\n",
        "from google.colab import files\n",
        "files.download(\"filled_dataset.xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ckPV-1JkaJG"
      },
      "source": [
        "Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-ZbauppkkqC"
      },
      "outputs": [],
      "source": [
        "df_encoded = df_filled.copy()\n",
        "\n",
        "# Determine the categorical variable columns that need to be encoded (excluding non-categorical, excluding time columns, etc.)\n",
        "exclude_cols = ['Todesdatum', 'Progress Datum', 'Nachsorge Datum', col1, col2]\n",
        "categorical_columns = [col for col in df_encoded.select_dtypes(include=['object', 'category']).columns if col not in exclude_cols]\n",
        "\n",
        "# encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoders = {}\n",
        "\n",
        "for col in categorical_columns:\n",
        "    le = LabelEncoder()\n",
        "    df_encoded[col] = df_encoded[col].astype(str).str.strip()         # remove space\n",
        "    df_encoded[col] = df_encoded[col].replace('nan', np.nan)          # transfer to NaN\n",
        "    df_encoded[col] = df_encoded[col].fillna('Missing')               # replace missing temporaly\n",
        "    df_encoded[col] = le.fit_transform(df_encoded[col])               # encoding\n",
        "    label_encoders[col] = le                                          # sace code\n",
        "\n",
        "\n",
        "df_encoded.to_excel(\"numerical.xlsx\", index=False)\n",
        "from google.colab import files\n",
        "files.download(\"numerical.xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sovvfaYhlL2i"
      },
      "source": [
        "Correlation Matrix Calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbrj5OQElH47"
      },
      "outputs": [],
      "source": [
        "numeric_df = df_encoded.select_dtypes(include=[np.number])\n",
        "if 'Wirkstoffschema adjuvante Therapie' in numeric_df.columns:\n",
        "    numeric_df = numeric_df.drop(columns=['Wirkstoffschema adjuvante Therapie'])\n",
        "\n",
        "\n",
        "\n",
        "def cramers_v(x, y):\n",
        "    confusion_matrix = pd.crosstab(x, y)\n",
        "    chi2 = chi2_contingency(confusion_matrix, correction=False)[0]\n",
        "    n = confusion_matrix.sum().sum()\n",
        "    phi2 = chi2 / n\n",
        "    r, k = confusion_matrix.shape\n",
        "    return np.sqrt(phi2 / min(k - 1, r - 1))\n",
        "\n",
        "# automatically select all categorical columns (excluding the target column)\n",
        "target_column = 'Wirkstoffschema neoadjuvante Therapie'\n",
        "\n",
        "categorical_cols = df_filled.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "if target_column in categorical_cols:\n",
        "    categorical_cols.remove(target_column)\n",
        "\n",
        "# Construct the Cramér’s V matrix and remove highly correlated columns (while retaining specified columns)\n",
        "target_cols_to_keep = {'Wirkstoffschema neoadjuvante Therapie', 'Wirkstoffschema adjuvante Therapie'}\n",
        "threshold = 0.75\n",
        "to_drop = set()\n",
        "\n",
        "cramer_matrix = pd.DataFrame(index=categorical_cols, columns=categorical_cols)\n",
        "\n",
        "for col1_iter, col2_iter in combinations(categorical_cols, 2):\n",
        "    if col1_iter == col2_iter:\n",
        "        cramer_matrix.loc[col1_iter, col2_iter] = 1.0\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        confusion_matrix = pd.crosstab(df_filled[col1_iter], df_filled[col2_iter])\n",
        "        if confusion_matrix.shape[0] < 2 or confusion_matrix.shape[1] < 2:\n",
        "            continue\n",
        "        chi2 = chi2_contingency(confusion_matrix, correction=False)[0]\n",
        "        n = confusion_matrix.sum().sum()\n",
        "        phi2 = chi2 / n\n",
        "        r, k = confusion_matrix.shape\n",
        "        denom = min(k - 1, r - 1)\n",
        "        if denom == 0:\n",
        "            continue\n",
        "        v = np.sqrt(phi2 / denom)\n",
        "        cramer_matrix.loc[col1_iter, col2_iter] = v\n",
        "        cramer_matrix.loc[col2_iter, col1_iter] = v\n",
        "        if v > threshold and col2_iter not in target_cols_to_keep:\n",
        "            to_drop.add(col2_iter)\n",
        "\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "# remove high relevalce columns\n",
        "categorical_cols = [col for col in categorical_cols if col not in to_drop]\n",
        "reduced_df = df_encoded.select_dtypes(include=[np.number])\n",
        "if target_column in reduced_df.columns:\n",
        "    reduced_df = reduced_df.drop(columns=[target_column])\n",
        "\n",
        "cramer_matrix = cramer_matrix.astype(float)\n",
        "print(f\"Dropped due to high correlation (>{threshold}): {sorted(to_drop)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COpGdQqsVmLU"
      },
      "source": [
        "All Correlation Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6v0-225KVowl"
      },
      "outputs": [],
      "source": [
        "cramer_pair_values = []\n",
        "for col1, col2 in combinations(categorical_cols, 2):\n",
        "    val = cramer_matrix.loc[col1, col2]\n",
        "    if pd.notna(val):\n",
        "        cramer_pair_values.append((col1, col2, float(val)))\n",
        "        cramer_pair_values.append((col2, col1, float(val)))\n",
        "\n",
        "cramer_values_df = pd.DataFrame(cramer_pair_values, columns=[\"Column 1\", \"Column 2\", \"Cramér's V\"])\n",
        "cramer_values_df.sort_values(by=\"Cramér's V\", ascending=False, inplace=True)\n",
        "\n",
        "print(\"All Cramér's V Correlation Values:\")\n",
        "print(cramer_values_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5Rn92mllXoS"
      },
      "source": [
        "Heat Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FU41xwzrlaaM"
      },
      "outputs": [],
      "source": [
        "\n",
        "# HEat Graph\n",
        "pivot_matrix = cramer_values_df.pivot(index='Column 1', columns='Column 2', values=\"Cramér's V\")\n",
        "\n",
        "for col1, col2, val in cramer_pair_values:\n",
        "    pivot_matrix.loc[col2, col1] = val\n",
        "for col in pivot_matrix.columns:\n",
        "    pivot_matrix.loc[col, col] = 1.0\n",
        "\n",
        "pivot_matrix = pivot_matrix.astype(float)\n",
        "\n",
        "plt.figure(figsize=(14, 12))\n",
        "sns.heatmap(pivot_matrix, annot=True, cmap=\"Reds\", fmt=\".2f\")\n",
        "plt.title(\"Cramér’s V Heatmap\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# # remove high relevalce columns\n",
        "\n",
        "categorical_cols = [col for col in categorical_cols if col not in to_drop]\n",
        "\n",
        "reduced_df = df_encoded.select_dtypes(include=[np.number])\n",
        "if target_column in reduced_df.columns:\n",
        "    reduced_df = reduced_df.drop(columns=[target_column])\n",
        "\n",
        "\n",
        "corr_matrix = reduced_df.corr(method='pearson').abs()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(14, 12))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(\"Correlation Heatmap of Reduced Numerical Features\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OROSO3R6lfpk"
      },
      "source": [
        "Lazy Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVLD8IxtlgbS"
      },
      "outputs": [],
      "source": [
        "# !pip install lazypredict\n",
        "col1 = 'Wirkstoffschema neoadjuvante Therapie'\n",
        "col2 = 'Wirkstoffschema adjuvante Therapie'\n",
        "target_column = col2\n",
        "from lazypredict.Supervised import LazyClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_encoded[reduced_df.columns]\n",
        "y = df_encoded[target_column]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "clf = LazyClassifier(verbose=0, ignore_warnings=True)\n",
        "models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
        "print(models)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XBdGVMVl5IN"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BfHT7zTDlsG"
      },
      "source": [
        "classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNGAwpH9o2bZ"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import seaborn as sns\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.ensemble import BaggingClassifier\n",
        "# from sklearn.preprocessing import StandardScaler, LabelEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFkTvYBhUTUo"
      },
      "source": [
        "Random Forest train and evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WbFnOr5Dpm8"
      },
      "outputs": [],
      "source": [
        "#\n",
        "\n",
        "def encode_target(df_encoded, target_column):\n",
        "    y = df_encoded[target_column]\n",
        "    le = LabelEncoder()\n",
        "    y_encoded = le.fit_transform(y)\n",
        "    return y_encoded, le\n",
        "\n",
        "def prepare_features(df_encoded, reduced_df, target_column=None):\n",
        "    X = df_encoded[reduced_df.columns]\n",
        "    if target_column and target_column in X.columns:\n",
        "        X = X.drop(columns=[target_column])\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    return pd.DataFrame(X_scaled, columns=X.columns)\n",
        "\n",
        "def split_data(X, y, test_size=0.4, random_state=42):\n",
        "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "def train_model(X_train, y_train, model_type='rf', random_state=42):\n",
        "    if model_type == 'rf':\n",
        "        model = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
        "    elif model_type == 'bagging':\n",
        "        model = BaggingClassifier(n_estimators=100, random_state=random_state)\n",
        "    else:\n",
        "        raise ValueError(\"model_type must be either 'rf' or 'bagging'\")\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    return y_pred, acc\n",
        "\n",
        "def plot_feature_importance(model, feature_names, top_n=5, title_prefix=\"\"):\n",
        "    if not hasattr(model, \"feature_importances_\"):\n",
        "        print(f\"{title_prefix}: Feature importance not available.\")\n",
        "        return pd.DataFrame()\n",
        "    importances = model.feature_importances_\n",
        "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
        "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(x='Importance', y='Feature', data=importance_df.head(top_n), palette='Reds_r')\n",
        "    plt.title(f\"{title_prefix} Top {top_n} Feature Importances\")\n",
        "    plt.xlabel(\"Importance Score\")\n",
        "    plt.ylabel(\"Feature Name\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return importance_df\n",
        "\n",
        "def export_results(importance_df, y_test, y_pred, label_encoder, target_column, model_tag=\"\"):\n",
        "    tag = f\"{target_column.replace(' ', '_')}_{model_tag}\"\n",
        "    feature_file = f\"features_{tag}.xlsx\"\n",
        "    prediction_file = f\"actual_vs_predicted_{tag}.xlsx\"\n",
        "\n",
        "    if not importance_df.empty:\n",
        "        importance_df.to_excel(feature_file, index=False)\n",
        "        files.download(feature_file)\n",
        "\n",
        "    y_test_decoded = label_encoder.inverse_transform(y_test)\n",
        "    y_pred_decoded = label_encoder.inverse_transform(y_pred)\n",
        "    comparison_df = pd.DataFrame({\n",
        "        'Actual Therapy': y_test_decoded,\n",
        "        'Predicted Therapy': y_pred_decoded\n",
        "    })\n",
        "    print(f\"\\n>>> {model_tag.upper()} Prediction Sample:\")\n",
        "    print(comparison_df.head(10))\n",
        "    comparison_df.to_excel(prediction_file, index=False)\n",
        "    files.download(prediction_file)\n",
        "\n",
        "    return feature_file, prediction_file, comparison_df\n",
        "\n",
        "def run_pipeline(df_filled, df_encoded, reduced_df, target_column, model_type='rf', test_size=0.4, top_n_features=5):\n",
        "    \"\"\"\n",
        "    Run classification for a single target_column using either Random Forest or Bagging.\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== Running {model_type.upper()} classification for: {target_column} ===\")\n",
        "    y_encoded, label_encoder = encode_target(df_encoded, target_column)\n",
        "    y = pd.Series(y_encoded).reset_index(drop=True)\n",
        "    X = prepare_features(df_encoded, reduced_df, target_column).reset_index(drop=True)\n",
        "    X_train, X_test, y_train, y_test = split_data(X, y, test_size=test_size)\n",
        "    model = train_model(X_train, y_train, model_type=model_type)\n",
        "    y_pred, acc = evaluate_model(model, X_test, y_test)\n",
        "    print(f\"{model_type.upper()} Accuracy: {acc:.4f}\")\n",
        "    importance_df = plot_feature_importance(model, X.columns, top_n=top_n_features, title_prefix=model_type.upper())\n",
        "    feature_file, prediction_file, comparison_df = export_results(\n",
        "        importance_df, y_test, y_pred, label_encoder, target_column, model_tag=model_type\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"feature_file\": feature_file,\n",
        "        \"prediction_file\": prediction_file,\n",
        "        \"importance_df\": importance_df,\n",
        "        \"comparison_df\": comparison_df\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtM2MyHkD84Y"
      },
      "source": [
        "First Theraphy as target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEzhv1CRDxDm"
      },
      "outputs": [],
      "source": [
        "# print(f\"col1 = {col1}\")\n",
        "# col1 = 'Wirkstoffschema neoadjuvante Therapie'\n",
        "# col2 = 'Wirkstoffschema adjuvante Therapie'\n",
        "# results = run_pipeline(\n",
        "#     df_filled=df_filled,\n",
        "#     df_encoded=df_encoded,\n",
        "#     reduced_df=reduced_df,\n",
        "#     target_column=col1,\n",
        "#     model_type='bagging',\n",
        "#     test_size=0.2,\n",
        "#     top_n_features=5\n",
        "# )\n",
        "# print(f\"col1 = {col1}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndMeYlLoUpNA"
      },
      "source": [
        "Second theraphy as target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dsews2GiUkj4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # # filter out the non-effective second therapy\n",
        "# # if col2 == 'Wirkstoffschema adjuvante Therapie':\n",
        "# #     mask = df_filled['Therapieerfolg neoadjuvante Therapie'].astype(str).str.strip().str.lower() != 'nein'\n",
        "# #     df_filled = df_filled[mask].reset_index(drop=True)\n",
        "# #     df_encoded = df_encoded[mask].reset_index(drop=True)\n",
        "# #     print(f\"Filtered for col2: {mask.sum()} rows retained (Therapieerfolg neoadjuvante Therapie ！= 'nein')\")\n",
        "\n",
        "# #run\n",
        "# results = run_pipeline(\n",
        "#     df_filled=df_filled,\n",
        "#     df_encoded=df_encoded,\n",
        "#     reduced_df=reduced_df,\n",
        "#     target_column= col2,\n",
        "#     model_type='rf',\n",
        "#     test_size=0.2,\n",
        "#     top_n_features=5\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QOtUKpU0gOK"
      },
      "source": [
        "Multi-Labeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKyQAAFNA8F4"
      },
      "outputs": [],
      "source": [
        "col1 = 'Wirkstoffschema neoadjuvante Therapie'\n",
        "col2 = 'Wirkstoffschema adjuvante Therapie'\n",
        "multi_target_cols = [col1, col2]\n",
        "\n",
        "y_raw = df_encoded[multi_target_cols].astype(str)\n",
        "ohe = OneHotEncoder(sparse_output=False)\n",
        "y = ohe.fit_transform(y_raw)\n",
        "\n",
        "y_df = pd.DataFrame(y, columns=ohe.get_feature_names_out(multi_target_cols))\n",
        "\n",
        "\n",
        "print(y_df.head(10).to_string(index=False))\n",
        "\n",
        "X = df_encoded.drop(columns=multi_target_cols)\n",
        "X = StandardScaler().fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "models = {\n",
        "    \"MultiOutput-RF\": MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    \"ClassifierChain-RF\": ClassifierChain(RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    # \"LabelPowerset-RF\": LabelPowerset(RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    \"LabelPowerset-RF\": LabelPowerset(XGBClassifier(\n",
        "        objective='binary:logistic',\n",
        "        eval_metric='logloss',\n",
        "        use_label_encoder=False,\n",
        "        random_state=42\n",
        "    )),\n",
        "} #try out different algorithms other than RF to compare accuracy\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    try:\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        if not isinstance(y_pred, np.ndarray):\n",
        "            y_pred = y_pred.toarray()\n",
        "\n",
        "        subset_acc = accuracy_score(y_test, y_pred)\n",
        "        hamming = hamming_loss(y_test, y_pred)\n",
        "\n",
        "        results.append({\n",
        "            'Model': name,\n",
        "            'Subset Accuracy': round(subset_acc, 4),\n",
        "            'Hamming Loss': round(hamming, 4)\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error in {name}: {e}\")\n",
        "        results.append({\n",
        "            'Model': name,\n",
        "            'Subset Accuracy': 'Error',\n",
        "            'Hamming Loss': 'Error'\n",
        "        })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n Multi-label Model Comparison:\")\n",
        "print(results_df)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUnO3qfajaoj"
      },
      "source": [
        "Apply LabelPowerset_RF for Significant Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YS_ZhNiHM9T1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "lp_model = LabelPowerset(RandomForestClassifier(n_estimators=100, random_state=42)) # Lable powerset model training\n",
        "lp_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "lp_importances = lp_model.classifier.feature_importances_ # extract significant features\n",
        "\n",
        "\n",
        "lp_feature_importance_df = pd.DataFrame({\n",
        "    'Feature': reduced_df.columns,\n",
        "    'Importance': lp_importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "\n",
        "print(\"Top 20 Important Features (from LabelPowerset-RF):\")\n",
        "print(lp_feature_importance_df.head(20).to_string(index=False)) #top 20\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='Importance', y='Feature', data=lp_feature_importance_df.head(20))\n",
        "plt.title(\"Top 20 Feature Importances from LabelPowerset-RF\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw144Y_Dz8Sb"
      },
      "source": [
        "optimizing hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YA-oLFy6jh9h"
      },
      "outputs": [],
      "source": [
        "def multilabel_score(y_true, y_pred):\n",
        "    return accuracy_score(y_true, y_pred)\n",
        "\n",
        "scorer = make_scorer(multilabel_score, greater_is_better=True)\n",
        "\n",
        "param_grid = {\n",
        "    'classifier__n_estimators': [50, 100, 200],\n",
        "    'classifier__max_depth': [None, 10, 20],\n",
        "    'classifier__min_samples_split': [2, 5],\n",
        "    'classifier__min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "lp_model = LabelPowerset(RandomForestClassifier(random_state=42))\n",
        "\n",
        "grid_search = GridSearchCV(estimator=lp_model,\n",
        "                           param_grid=param_grid,\n",
        "                           scoring=scorer,\n",
        "                           cv=3,\n",
        "                           verbose=2)\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Accuracy Score:\", grid_search.best_score_)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"Final Test Subset Accuracy:\", round(accuracy_score(y_test, y_pred), 4))\n",
        "print(\"Final Test Hamming Loss:\", round(hamming_loss(y_test, y_pred), 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfjVgS7anTBU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGieLsA0xOCIEP3qoBb21g",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.11.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}